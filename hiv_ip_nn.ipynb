{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataframe with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, CSV\n",
    "df = CSV.read(\"hiv_pam.csv\", DataFrame);\n",
    "println(\"$df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "symbolic mathematical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NeuralPDE, Flux, ModelingToolkit, Optimization, OptimizationOptimJL, OrdinaryDiffEq, Plots\n",
    "import ModelingToolkit: Interval, infimum, supremum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ModelingToolkit\n",
    "import ModelingToolkit: Interval, infimum, supremum\n",
    "\n",
    "ε_1, ε_2, a_A, a_T = 0.0, 0.0,0.0, 0.008\n",
    "u = 0.0; rho_1 = 1.0; rho_2 = 1.0; \n",
    "@parameters t, d_1, δ, d_2, f, ε_2, a_A, a_T, a_E, γ_E, \n",
    "                δ_E2, ε_1, m, K_V, γ_T, c, d_E, δ_E1, p_T, \n",
    "                p_E, b_E2, k_1, k_2, λ_T, K_S, N_T, λ_E, K_γ, \n",
    "                K_b1, K_b2, K_d, b_E1;\n",
    "\n",
    "@variables T_1(..), Ts_1(..), T_2(..), Ts_2(..), V_1(..), V_2(..), E_1(..), E_2(..) \n",
    "Dt = Differential(t)\n",
    "\n",
    "eqs=[\n",
    "        Dt(T_1(t))~-d_1*T_1(t)-(1-u*ε_1)*k_1*V_1(t)*T_1(t)-γ_T*T_1(t)+p_T*(a_T*V_1(t))/(V_1(t)+K_V)*T_2(t)+p_T*a_A*T_2(t),\n",
    "        Dt(Ts_1(t))~(1-u*ε_1)*k_1*V_1(t)*T_1(t)-δ*Ts_1(t)-m*E_1(t)*Ts_1(t)-γ_T*Ts_1(t)+p_T*(a_T*V_1(t))/(V_1(t)+K_V)*Ts_2(t)+p_T*a_A*Ts_2(t),\n",
    "        Dt(T_2(t))~λ_T*K_S/(V_1(t)+K_S)+γ_T*T_1(t)-d_2*T_2(t)-(1-f*u*ε_1)*k_2*V_1(t)*T_2(t)-(a_T*V_1(t))/(V_1(t)+K_V)*T_2(t)-a_A*T_2(t),\n",
    "        Dt(Ts_2(t))~γ_T*Ts_1(t)+(1-f*u*ε_1)*k_2*V_1(t)*T_2(t)-d_2*T_2(t)-(a_T*V_1(t))/(V_1(t)+K_V)*Ts_2(t)-a_A*Ts_2(t),\n",
    "        Dt(V_1(t))~(1-u*ε_2)*1000.0*N_T*δ*Ts_1(t)-c*V_1(t)-1000.0*((1-u*ε_1)*rho_1*k_1*T_1(t)+(1-f*u*ε_1)*rho_2*k_2*T_2(t))*V_1(t),\n",
    "        Dt(V_2(t))~u*ε_2*1000.0*N_T*δ*Ts_1(t)-c*V_2(t),\n",
    "        Dt(E_1(t))~λ_E+b_E1*Ts_1(t)*E_1(t)/(Ts_1(t)+K_b1)-d_E*Ts_1(t)*E_1(t)/(Ts_1(t)+K_d)-δ_E1*E_1(t)-γ_E*(T_1(t)+Ts_1(t))*E_1(t)/(T_1(t)+Ts_1(t)+K_γ)+p_E*a_E*V_1(t)*E_2(t)/(V_1(t)+K_V),\n",
    "        Dt(E_2(t))~γ_E*(T_1(t)+Ts_1(t))*E_1(t)/(T_1(t)+Ts_1(t)+K_γ)+b_E2*K_b2*E_2(t)/(E_2(t)+K_b2)-δ_E2*E_2(t)-a_E*V_1(t)*E_2(t)/(V_1(t)+K_V)\n",
    "]\n",
    "\n",
    "sim = [T_1(0),Ts_1(0),T_2(0),Ts_2(0),V_1(0),V_2(0),E_1(0),E_2(0)];\n",
    "init_df = [13.13,5.330, 424.4, 2.984, 10., 0., 0.04574, 0.002863]\n",
    "bcs = [s ~ i for (s,i) in zip(sim,init_df)]\n",
    "domains = [t ∈ Interval(0.0,100.0)]\n",
    "dt = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = [d_1, δ, d_2, f, ε_2, a_A, a_T, a_E, γ_E, \n",
    "δ_E2, ε_1, m, K_V, γ_T, c, d_E, δ_E1, p_T, \n",
    "p_E, b_E2, k_1, k_2, λ_T, K_S, N_T, λ_E, K_γ, \n",
    "K_b1, K_b2, K_d, b_E1];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creat modelling_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_hiv (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function model_hiv(dX,X,p,t)\n",
    "\n",
    "    rho1 = 1.; rho2 = 1.;\n",
    "    ksi1 = p[11]*p[32]; ksi2 = p[5]*p[32];     \n",
    "    \n",
    "      dX[1] = -p[1] * X[1] - (1 - ksi1) * p[21] * X[5] * X[1] - p[14] * X[1] + p[18] * (p[7] *(X[5]/(X[5]+p[13]))+p[6]) * X[3];  #dT1\n",
    "        \n",
    "      dX[2] = (1. - ksi1) * p[21] * X[5] * X[1] - p[2] * X[2] - p[12] * X[7] * X[2] - p[14] * X[2] + p[18] * (p[7] * (X[5]/(X[5]+p[13])) + p[6]) * X[4]; #dT*_1\n",
    "        \n",
    "      dX[3] = p[23] * (p[24]/(X[5]+p[24])) + p[14] * X[1] - p[3] * X[3] - (1. - p[4] * ksi1)  * p[22] * X[5] * X[3] - (p[7] * (X[5]/(X[5]+p[13])) + p[6]) * X[3]; #dT2\n",
    "      \n",
    "      dX[4] = p[14] * X[2] + (1 - p[4] * ksi1)  * p[22] * X[5] * X[3] - p[3] * X[4] - (p[7] * (X[5]/(X[5]+p[13])) + p[6]) * X[4]; #dT_2\n",
    "        \n",
    "      dX[5] = (1. - ksi2) * 10^3 * p[25] * p[2] * X[2] - p[15] * X[5] - 10^3 * ((1 - ksi1) * rho1 * p[21] * X[1] * X[5]) - 10^3 * ((1. - p[4] * ksi1) * rho2 * p[22] * X[3] * X[5]) ; #dVI\n",
    "        \n",
    "      dX[6] = ksi2 * 10^3 * p[25] * p[2] * X[2] - p[15] * X[6]; #dVNI\n",
    "        \n",
    "      dX[7] = p[26] + p[31] * (X[2]/(X[2]+p[28])) * X[7] - p[16] * (X[2]/(X[2]+p[30])) * X[7] - p[17] * X[7] - p[9] * ((X[1]+X[2])/(X[1]+X[2]+p[27])) * X[7] + p[19] * p[8] * (X[5]/(X[5]+p[13])) * X[8]; #p[16]1\n",
    "        \n",
    "      dX[8] = p[9] * ((X[1]+X[2])/(X[1]+X[2]+p[27])) * X[7] + p[20] * (p[29]/(X[8]+p[29]))* X[8] - p[10] * X[8] - p[8] * (X[5]/(X[5]+p[13])) * X[8];\n",
    "     \n",
    "     \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, CSV\n",
    "df = CSV.read(\"hiv_pam.csv\", DataFrame);\n",
    "df.value2[11] = 0.0\n",
    "pam = df.value2;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Fix\n",
    "    value \n",
    "    index\n",
    "end\n",
    "\n",
    "function fix_parameters(dt::DataFrame,find_parameter)# what pust dt::?\n",
    "\n",
    "    indexes = filter(i->dt[i,1] ∉ find_parameter, range(1,size(dt,1)))\n",
    "    value_fix = dt[indexes,2]\n",
    "    fil_set = filter(i->dt[i,1] in find_parameter, range(1,size(dt,1)));\n",
    "    pam = dt[fil_set,2]\n",
    "    return (pam,Fix(value_fix,indexes))\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "function CollectVector(q,fix_pam::Fix)\n",
    "    p = [];\n",
    "    p = append!(p,q)\n",
    "    if length(fix_pam.value)!=0 \n",
    "        for (i,j) in zip(fix_pam.index,fix_pam.value)\n",
    "        p = insert!(p,i,j) \n",
    "        end\n",
    "    end\n",
    "    return p\n",
    "end\n",
    "\n",
    "using DifferentialEquations\n",
    "struct ODEs\n",
    "    model::Function\n",
    "    ID\n",
    "    time\n",
    "    step\n",
    "  end\n",
    "  \n",
    "  \n",
    "function solve_ODE(ode::ODEs,q; fix_pam=Fix([],[]), method=Tsit5()) #fix_pam = fix, ind = indexes)\n",
    "    \n",
    "    pam = CollectVector(q,fix_pam)\n",
    "    pam = append!(pam,1.0)\n",
    "    prob = ODEProblem(ode.model,ode.ID,ode.time,pam)\n",
    "    data =  solve(prob,method,saveat=ode.step);\n",
    "    return data\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = [13.13,5.330, 424.4, 2.984, 10., 0., 0.04574, 0.002863]\n",
    "t_start = 0.0; t_end = 100.0; step = 1.0\n",
    "tspan = [t_start, t_end]\n",
    "modelling_data = solve_ODE(ODEs(model_hiv,ID,tspan,step),pam);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = [infimum(d.domain):dt:supremum(d.domain) for d in domains][1]\n",
    "function getData(sol)\n",
    "    data = []\n",
    "    us = hcat(sol(ts).u...)\n",
    "    ts_ = hcat(sol(ts).t...)\n",
    "    return [us,ts_]\n",
    "end\n",
    "data = getData(modelling_data)\n",
    "\n",
    "(u_ , t_) = data\n",
    "len = length(data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = modelling_data\n",
    "T = data.t\n",
    "Total_CD4 = map(i->(data[1,i]+data[2,i]+data[3,i]+data[4,i]),range(1,length(data[1,:])))\n",
    "VL = map(i->(data[5,i]+data[6,i]), range(1,length(data.t)));\n",
    "VL1 = log10.(VL);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "plot(data.t,Total_CD4, ylabel = \"CD4 T-клетки/мкл\", label =\"смоделированные данные\", markershapes = :o, markersize = 3,c=:blue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#savefig(\"md_TTcell.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(data.t, VL,  xlabel = \"t(дни)\",c=:red,markershapes = :o, markersize = 3, ylabel = \"вирусные копии/мл\", label =\"смоделированные данные\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#savefig(\"md_VL.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "upload medical-experemental data for u_\n",
    "- Virual Load data\n",
    "- Total T-cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, CSV\n",
    "name_Time = [\"Pre-infection\",\n",
    "    \"0 Weeks\",\n",
    "    \"1 Weeks\",\n",
    "    \"2 Weeks\",\n",
    "    \"3 Weeks\",\n",
    "    \"4 Weeks\",\n",
    "    \"6 Months\",\n",
    "    \"1 Year\"]\n",
    "    \n",
    "val_Time =[0.0,\n",
    "    7.0,\n",
    "    14.0,\n",
    "    21.0,\n",
    "    28.0,\n",
    "    35.0,\n",
    "    180.0,\n",
    "    365.0]\n",
    "#P1\n",
    "val_VL = [0.0, 5.89, 7.96,6.3,5.91,5.87,4.5,5.0]\n",
    "val_CD4 = [1200.,890., 600.,420.,550.,520.,560.,580.]\n",
    "\n",
    "meas_data = DataFrame(name=name_Time, day=val_Time, P1_VL=val_VL, P1_CD4=val_CD4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_function for nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len - это # количество точек измерений время час день т д \n",
    "#depvars = [:T_1,:Ts_1,:T_2,:Ts_2,:V_1,:V_2,:E_1,:E_2]\n",
    "#function additional_loss(phi, θ , p)\n",
    "    #return sum(sum(abs2, phi[i](t_ , θ[depvars[i]]) .- u_[[i], :])/len for i in 1:1:8)\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = [Flux.destructure(c)[1] for c in [chain1,chain2,chain3]]\n",
    "acum =  [0;accumulate(+, length.(init_params))]\n",
    "sep = [acum[i]+1 : acum[i+1] for i in 1:length(acum)-1]\n",
    "(u_ , t_) = data\n",
    "len = length(data[2])\n",
    "\n",
    "function additional_loss(phi, θ , p)\n",
    "    return sum(sum(abs2, phi[i](t_ , θ[sep[i]]) .- u_[[i], :])/len for i in 1:1:3)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "additional_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Vector{Chain{Tuple{Dense{typeof(σ), Matrix{Float32}, Vector{Float32}}, Dense{typeof(σ), Matrix{Float32}, Vector{Float32}}, Dense{typeof(σ), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}}:\n",
       " Chain(Dense(1 => 8, σ), Dense(8 => 8, σ), Dense(8 => 8, σ), Dense(8 => 1))  \u001b[90m# 169 parameters\u001b[39m\n",
       " Chain(Dense(1 => 8, σ), Dense(8 => 8, σ), Dense(8 => 8, σ), Dense(8 => 1))  \u001b[90m# 169 parameters\u001b[39m\n",
       " Chain(Dense(1 => 8, σ), Dense(8 => 8, σ), Dense(8 => 8, σ), Dense(8 => 1))  \u001b[90m# 169 parameters\u001b[39m\n",
       " Chain(Dense(1 => 8, σ), Dense(8 => 8, σ), Dense(8 => 8, σ), Dense(8 => 1))  \u001b[90m# 169 parameters\u001b[39m\n",
       " Chain(Dense(1 => 8, σ), Dense(8 => 8, σ), Dense(8 => 8, σ), Dense(8 => 1))  \u001b[90m# 169 parameters\u001b[39m\n",
       " Chain(Dense(1 => 8, σ), Dense(8 => 8, σ), Dense(8 => 8, σ), Dense(8 => 1))  \u001b[90m# 169 parameters\u001b[39m\n",
       " Chain(Dense(1 => 8, σ), Dense(8 => 8, σ), Dense(8 => 8, σ), Dense(8 => 1))  \u001b[90m# 169 parameters\u001b[39m\n",
       " Chain(Dense(1 => 8, σ), Dense(8 => 8, σ), Dense(8 => 8, σ), Dense(8 => 1))  \u001b[90m# 169 parameters\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = length(domains)\n",
    "n = 8\n",
    "chain1 = Flux.Chain(Dense(input_,n,Flux.σ),Dense(n,n,Flux.σ),Dense(n,n,Flux.σ),Dense(n,1))\n",
    "chain2 = Flux.Chain(Dense(input_,n,Flux.σ),Dense(n,n,Flux.σ),Dense(n,n,Flux.σ),Dense(n,1))\n",
    "chain3 = Flux.Chain(Dense(input_,n,Flux.σ),Dense(n,n,Flux.σ),Dense(n,n,Flux.σ),Dense(n,1))\n",
    "chain4 = Flux.Chain(Dense(input_,n,Flux.σ),Dense(n,n,Flux.σ),Dense(n,n,Flux.σ),Dense(n,1))\n",
    "chain5 = Flux.Chain(Dense(input_,n,Flux.σ),Dense(n,n,Flux.σ),Dense(n,n,Flux.σ),Dense(n,1))\n",
    "chain6 = Flux.Chain(Dense(input_,n,Flux.σ),Dense(n,n,Flux.σ),Dense(n,n,Flux.σ),Dense(n,1))\n",
    "chain7 = Flux.Chain(Dense(input_,n,Flux.σ),Dense(n,n,Flux.σ),Dense(n,n,Flux.σ),Dense(n,1))\n",
    "chain8 = Flux.Chain(Dense(input_,n,Flux.σ),Dense(n,n,Flux.σ),Dense(n,n,Flux.σ),Dense(n,1))\n",
    "\n",
    "chain = [chain1 , chain2, chain3, chain4, chain5, chain6, chain7, chain8]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "additional_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_params = [Flux.destructure(c)[1] for c in chain]\n",
    "acum =  [0;accumulate(+, length.(init_params))]\n",
    "sep = [acum[i]+1 : acum[i+1] for i in 1:length(acum)-1]\n",
    "(u_ , t_) = data\n",
    "len = length(data[2])\n",
    "\n",
    "\n",
    "function additional_loss(phi, θ , p)\n",
    "    Phi(i) = phi[i](t_ , θ[sep[i]])\n",
    "    data_U(i) = u_[[i], :]\n",
    "    sum_4(x) = x(1) .+ x(2) .+ x(3) .+ x(4)\n",
    "    sum_56(x) = x(5) .+ x(6)\n",
    "    return sum(abs2, sum_4(Phi) .- sum_4(data_U))/len + sum(abs2, sum_56(Phi) .- sum_56(data_U))/len\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NeuralPDE, Lux, CUDA, Random\n",
    "using Optimization\n",
    "using OptimizationOptimisers\n",
    "import ModelingToolkit: Interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ComponentArrays, CUDA, Flux, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: initialparameters not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: initialparameters not defined",
      "",
      "Stacktrace:",
      " [1] getproperty(x::Module, f::Symbol)",
      "   @ Base .\\Base.jl:31",
      " [2] top-level scope",
      "   @ In[14]:3",
      " [3] eval",
      "   @ .\\boot.jl:368 [inlined]",
      " [4] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1428"
     ]
    }
   ],
   "source": [
    "names = :T_1,:Ts_1,:T_2,:Ts_2,:V_1,:V_2,:E_1,:E_2\n",
    "\n",
    "init_params = Flux.initialparameters(Random.default_rng(),\n",
    "                                     chain)\n",
    "init_params = NamedTuple{names}(init_params)\n",
    "init_params = ComponentArray(init_params)\n",
    "ps = Float64.(gpu(init_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ComponentArrays, CUDA, Lux, Random\n",
    "\n",
    "# [...]\n",
    "\n",
    "@parameters x y\n",
    "@variables f1(..) f2(..) f3(..) f4(..)\n",
    "\n",
    "# [...]\n",
    "\n",
    "chain = [chain1 , chain2, chain3, chain4]\n",
    "names = :f1, :f2, :f3, :f4  # same as the variables from the beginning\n",
    "\n",
    "init_params = Lux.initialparameters.(Random.default_rng(),\n",
    "                                     chain)\n",
    "init_params = NamedTuple{names}(init_params)\n",
    "init_params = ComponentArray(init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = Float64.(gpu(init_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\begin{equation}\n",
       "\\left[\n",
       "\\begin{array}{c}\n",
       "T_{1}\\left( t \\right) \\\\\n",
       "\\mathrm{Ts}_{1}\\left( t \\right) \\\\\n",
       "T_{2}\\left( t \\right) \\\\\n",
       "\\mathrm{Ts}_{2}\\left( t \\right) \\\\\n",
       "V_{1}\\left( t \\right) \\\\\n",
       "V_{2}\\left( t \\right) \\\\\n",
       "E_{1}\\left( t \\right) \\\\\n",
       "E_{2}\\left( t \\right) \\\\\n",
       "\\end{array}\n",
       "\\right]\n",
       "\\end{equation}\n",
       " $$"
      ],
      "text/plain": [
       "8-element Vector{Num}:\n",
       "  T_1(t)\n",
       " Ts_1(t)\n",
       "  T_2(t)\n",
       " Ts_2(t)\n",
       "  V_1(t)\n",
       "  V_2(t)\n",
       "        E_1(t)\n",
       "        E_2(t)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = [T_1(t),Ts_1(t),T_2(t),Ts_2(t),V_1(t),V_2(t),E_1(t),E_2(t)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#20 (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discretization = NeuralPDE.PhysicsInformedNN(chain,\n",
    "                                             NeuralPDE.GridTraining(dt),\n",
    "                                             param_estim=true,\n",
    "                                             #init_params =init_params,\n",
    "                                             additional_loss=additional_loss)\n",
    "                                             \n",
    "@named pde_system = PDESystem(eqs,bcs,domains,[t],var,sp, defaults=Dict([p .=> 1.0 for p in sp]))\n",
    "prob = NeuralPDE.discretize(pde_system,discretization)\n",
    "callback = function (p,l)\n",
    "    println(\"Current loss is: $l\")\n",
    "    return false\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss is: 2.9431466738447084e9\n",
      "Current loss is: 2.94253469910049e9\n",
      "Current loss is: 2.9422119838031077e9\n",
      "Current loss is: 2.942174074023358e9\n",
      "Current loss is: 2.941007152574911e9\n",
      "Current loss is: 2.9409964309806824e9\n",
      "Current loss is: 2.9380227951639156e9\n",
      "Current loss is: 2.9346233957675085e9\n",
      "Current loss is: 2.924590578000176e9\n",
      "Current loss is: 2.9200371098488417e9\n",
      "Current loss is: 2.919760225759012e9\n",
      "Current loss is: 2.918950332426031e9\n",
      "Current loss is: 2.910605572049051e9\n",
      "Current loss is: 2.901746538076331e9\n",
      "Current loss is: 2.9002053731525903e9\n",
      "Current loss is: 2.8992484710777497e9\n",
      "Current loss is: 2.896487488005799e9\n",
      "Current loss is: 2.8926410001696663e9\n",
      "Current loss is: 2.8859828883881764e9\n",
      "Current loss is: 2.8839057426701913e9\n",
      "Current loss is: 2.876688630832036e9\n",
      "Current loss is: 2.876148367647539e9\n",
      "Current loss is: 2.8721572892220945e9\n",
      "Current loss is: 2.8688830994911413e9\n",
      "Current loss is: 2.8672974757186036e9\n",
      "Current loss is: 2.8649668867300305e9\n",
      "Current loss is: 2.864372573974462e9\n",
      "Current loss is: 2.8616421133059325e9\n",
      "Current loss is: 2.858772669272343e9\n",
      "Current loss is: 2.858234486699378e9\n",
      "Current loss is: 2.8573033982345243e9\n",
      "Current loss is: 2.8558861989919e9\n",
      "Current loss is: 2.8513488222954326e9\n",
      "Current loss is: 2.849278249545416e9\n",
      "Current loss is: 2.8486561161733556e9\n",
      "Current loss is: 2.8479443099358206e9\n",
      "Current loss is: 2.846585774669822e9\n",
      "Current loss is: 2.846399638595959e9\n",
      "Current loss is: 2.8461126527256255e9\n",
      "Current loss is: 2.8455871422232056e9\n",
      "Current loss is: 2.84495672876165e9\n",
      "Current loss is: 2.844915363715635e9\n",
      "Current loss is: 2.84439119689777e9\n",
      "Current loss is: 2.8377132856228666e9\n",
      "Current loss is: 2.836050205289746e9\n",
      "Current loss is: 2.8347511039788914e9\n",
      "Current loss is: 2.831273397251164e9\n",
      "Current loss is: 2.8262028785895214e9\n",
      "Current loss is: 2.8229288287252355e9\n",
      "Current loss is: 2.8156731527681646e9\n",
      "Current loss is: 2.81478019777195e9\n",
      "Current loss is: 2.8051186004015107e9\n",
      "Current loss is: 2.802779012758899e9\n",
      "Current loss is: 2.7956219921372375e9\n",
      "Current loss is: 2.795360881685775e9\n",
      "Current loss is: 2.791470092858226e9\n",
      "Current loss is: 2.788953268547544e9\n",
      "Current loss is: 2.784885758158452e9\n",
      "Current loss is: 2.7825628521073155e9\n",
      "Current loss is: 2.782425026409356e9\n",
      "Current loss is: 2.766386683274379e9\n",
      "Current loss is: 2.765025922693221e9\n",
      "Current loss is: 2.7579252559682136e9\n",
      "Current loss is: 2.750975062106522e9\n",
      "Current loss is: 2.740299805362625e9\n",
      "Current loss is: 2.737806772917945e9\n",
      "Current loss is: 2.7252781638152933e9\n",
      "Current loss is: 2.7198795732615314e9\n",
      "Current loss is: 2.69881958279915e9\n",
      "Current loss is: 2.688842451089224e9\n",
      "Current loss is: 2.685453385848943e9\n",
      "Current loss is: 2.678114964323907e9\n",
      "Current loss is: 2.667558749556026e9\n",
      "Current loss is: 2.6594869427489743e9\n",
      "Current loss is: 2.657340956541611e9\n",
      "Current loss is: 2.65205961514534e9\n",
      "Current loss is: 2.6504000254597898e9\n",
      "Current loss is: 2.6338165187572575e9\n",
      "Current loss is: 2.6204007199430947e9\n",
      "Current loss is: 2.6174147430308294e9\n",
      "Current loss is: 2.617386630376605e9\n",
      "Current loss is: 2.6173308058552017e9\n",
      "Current loss is: 2.616447193287148e9\n",
      "Current loss is: 2.615373209316105e9\n",
      "Current loss is: 2.6152701053589497e9\n",
      "Current loss is: 2.6151705648753543e9\n",
      "Current loss is: 2.615036906243316e9\n",
      "Current loss is: 2.6148681668154545e9\n",
      "Current loss is: 2.614708992045459e9\n",
      "Current loss is: 2.613790270934123e9\n",
      "Current loss is: 2.6133257725814404e9\n",
      "Current loss is: 2.61231464135809e9\n",
      "Current loss is: 2.6119925343379254e9\n",
      "Current loss is: 2.611940283428529e9\n",
      "Current loss is: 2.611915923887852e9\n",
      "Current loss is: 2.6118778741601167e9\n",
      "Current loss is: 2.61170160085083e9\n",
      "Current loss is: 2.6113151761776066e9\n",
      "Current loss is: 2.610829775112541e9\n",
      "Current loss is: 2.6106056672287064e9\n",
      "Current loss is: 2.6105820378669443e9\n",
      "Current loss is: 2.6102592059717164e9\n",
      "Current loss is: 2.610141944127969e9\n",
      "Current loss is: 2.6100887347052574e9\n",
      "Current loss is: 2.6100685059238076e9\n",
      "Current loss is: 2.6100536157279253e9\n",
      "Current loss is: 2.6099886277835193e9\n",
      "Current loss is: 2.609982251671054e9\n",
      "Current loss is: 2.6099539291102724e9\n",
      "Current loss is: 2.6097775129828877e9\n",
      "Current loss is: 2.609762638762556e9\n",
      "Current loss is: 2.60970127652419e9\n",
      "Current loss is: 2.6096797992596765e9\n",
      "Current loss is: 2.6096228529163113e9\n",
      "Current loss is: 2.609611095036952e9\n",
      "Current loss is: 2.609575606899121e9\n",
      "Current loss is: 2.609570635849045e9\n",
      "Current loss is: 2.6095653517000017e9\n",
      "Current loss is: 2.6095520739700274e9\n",
      "Current loss is: 2.609543120632148e9\n",
      "Current loss is: 2.609521178549291e9\n",
      "Current loss is: 2.6094988298251376e9\n",
      "Current loss is: 2.6094893329817777e9\n",
      "Current loss is: 2.609457096401619e9\n",
      "Current loss is: 2.609113552111292e9\n",
      "Current loss is: 2.6090752895014243e9\n",
      "Current loss is: 2.6090239382100754e9\n",
      "Current loss is: 2.608894163863326e9\n",
      "Current loss is: 2.608872965993254e9\n",
      "Current loss is: 2.6087861122853465e9\n",
      "Current loss is: 2.608605702104407e9\n",
      "Current loss is: 2.608447029907428e9\n",
      "Current loss is: 2.608226690688727e9\n",
      "Current loss is: 2.608110043676703e9\n",
      "Current loss is: 2.6079643278840766e9\n",
      "Current loss is: 2.6078655336489596e9\n",
      "Current loss is: 2.607325654932613e9\n",
      "Current loss is: 2.607243304561823e9\n",
      "Current loss is: 2.6072074914796104e9\n",
      "Current loss is: 2.607195912613183e9\n",
      "Current loss is: 2.607190378513168e9\n",
      "Current loss is: 2.606757582832106e9\n",
      "Current loss is: 2.6067529963800483e9\n",
      "Current loss is: 2.606702022313808e9\n",
      "Current loss is: 2.60658196737168e9\n",
      "Current loss is: 2.6065548236815753e9\n",
      "Current loss is: 2.6065116199514194e9\n",
      "Current loss is: 2.606500698454219e9\n",
      "Current loss is: 2.606475395803156e9\n",
      "Current loss is: 2.606054728945805e9\n",
      "Current loss is: 2.606045412328001e9\n",
      "Current loss is: 2.606002198363035e9\n",
      "Current loss is: 2.605979137116367e9\n",
      "Current loss is: 2.605978611065793e9\n",
      "Current loss is: 2.6059272455010896e9\n",
      "Current loss is: 2.605893538330255e9\n",
      "Current loss is: 2.605780761029269e9\n",
      "Current loss is: 2.605656562631704e9\n",
      "Current loss is: 2.6055925616782665e9\n",
      "Current loss is: 2.6054573349305596e9\n",
      "Current loss is: 2.605445162916124e9\n",
      "Current loss is: 2.605434665830927e9\n",
      "Current loss is: 2.605409128407765e9\n",
      "Current loss is: 2.605374960599561e9\n",
      "Current loss is: 2.605350770740397e9\n",
      "Current loss is: 2.605322020714743e9\n",
      "Current loss is: 2.6053125037412195e9\n",
      "Current loss is: 2.605310795318414e9\n",
      "Current loss is: 2.6053094294010677e9\n",
      "Current loss is: 2.6052999215355487e9\n",
      "Current loss is: 2.605285335071889e9\n",
      "Current loss is: 2.605254599471856e9\n",
      "Current loss is: 2.6052433553037467e9\n",
      "Current loss is: 2.605242813976717e9\n",
      "Current loss is: 2.6052379960376444e9\n",
      "Current loss is: 2.605232229648033e9\n",
      "Current loss is: 2.605212347083067e9\n",
      "Current loss is: 2.6051841441471415e9\n",
      "Current loss is: 2.605160340239716e9\n",
      "Current loss is: 2.605136508636166e9\n",
      "Current loss is: 2.6050807255437627e9\n",
      "Current loss is: 2.605061187857477e9\n",
      "Current loss is: 2.605043270650302e9\n",
      "Current loss is: 2.604935003218696e9\n",
      "Current loss is: 2.604845185250298e9\n",
      "Current loss is: 2.6048225184534984e9\n",
      "Current loss is: 2.6047258323205824e9\n",
      "Current loss is: 2.6046398802689753e9\n",
      "Current loss is: 2.6046273763865285e9\n",
      "Current loss is: 2.6046238232566485e9\n",
      "Current loss is: 2.604505073528134e9\n",
      "Current loss is: 2.604420907468954e9\n",
      "Current loss is: 2.6044164184136577e9\n",
      "Current loss is: 2.6043595491873465e9\n",
      "Current loss is: 2.604309861690806e9\n",
      "Current loss is: 2.604301608492154e9\n",
      "Current loss is: 2.604295733478832e9\n",
      "Current loss is: 2.6042667441924353e9\n",
      "Current loss is: 2.6042565375161037e9\n",
      "Current loss is: 2.604255917935004e9\n",
      "Current loss is: 2.6042447888204784e9\n",
      "Current loss is: 2.604241660534571e9\n",
      "Current loss is: 2.6042365903667088e9\n",
      "Current loss is: 2.6041702735530386e9\n",
      "Current loss is: 2.6041096463114314e9\n",
      "Current loss is: 2.6040617934817653e9\n",
      "Current loss is: 2.604038389251549e9\n",
      "Current loss is: 2.603989324750464e9\n",
      "Current loss is: 2.6039253562484183e9\n",
      "Current loss is: 2.603882531382805e9\n",
      "Current loss is: 2.603882132453237e9\n",
      "Current loss is: 2.6038776045838404e9\n",
      "Current loss is: 2.6038660015265217e9\n",
      "Current loss is: 2.6037990240289025e9\n",
      "Current loss is: 2.6037745521648464e9\n",
      "Current loss is: 2.6037360186439466e9\n",
      "Current loss is: 2.603711230551447e9\n",
      "Current loss is: 2.6035980512809353e9\n",
      "Current loss is: 2.603542108481738e9\n",
      "Current loss is: 2.603446168359874e9\n",
      "Current loss is: 2.6031060782793427e9\n",
      "Current loss is: 2.603049084121099e9\n",
      "Current loss is: 2.602957595470404e9\n",
      "Current loss is: 2.6028344084170313e9\n",
      "Current loss is: 2.602729341179117e9\n",
      "Current loss is: 2.60266912454876e9\n",
      "Current loss is: 2.602668152125276e9\n",
      "Current loss is: 2.6026352837463818e9\n",
      "Current loss is: 2.602506182608054e9\n",
      "Current loss is: 2.6025016000945764e9\n",
      "Current loss is: 2.6024905669384327e9\n",
      "Current loss is: 2.602460374467638e9\n",
      "Current loss is: 2.6024300745271196e9\n",
      "Current loss is: 2.602093596396211e9\n",
      "Current loss is: 2.601935426968177e9\n",
      "Current loss is: 2.6017186778838468e9\n",
      "Current loss is: 2.6014884953242617e9\n",
      "Current loss is: 2.6013766345225754e9\n",
      "Current loss is: 2.6012227328071733e9\n",
      "Current loss is: 2.6011049316031957e9\n",
      "Current loss is: 2.6007819685057044e9\n",
      "Current loss is: 2.600649909207705e9\n",
      "Current loss is: 2.6000347327153225e9\n",
      "Current loss is: 2.599600888018769e9\n",
      "Current loss is: 2.5987316822192926e9\n",
      "Current loss is: 2.5977336304485164e9\n",
      "Current loss is: 2.597637871671101e9\n",
      "Current loss is: 2.5976131857068114e9\n",
      "Current loss is: 2.597612733758194e9\n",
      "Current loss is: 2.597582884352572e9\n",
      "Current loss is: 2.5975745161516623e9\n",
      "Current loss is: 2.597537202855699e9\n",
      "Current loss is: 2.5975246912097554e9\n",
      "Current loss is: 2.597404873568958e9\n",
      "Current loss is: 2.5973715942021194e9\n",
      "Current loss is: 2.597273719542783e9\n",
      "Current loss is: 2.597251237547104e9\n",
      "Current loss is: 2.597250003635683e9\n",
      "Current loss is: 2.5971912858083334e9\n",
      "Current loss is: 2.5971445775284157e9\n",
      "Current loss is: 2.597143743588384e9\n",
      "Current loss is: 2.597139382411081e9\n",
      "Current loss is: 2.597119029287523e9\n",
      "Current loss is: 2.597093293697023e9\n",
      "Current loss is: 2.596736326926355e9\n",
      "Current loss is: 2.5967032987690806e9\n",
      "Current loss is: 2.5966259791297684e9\n",
      "Current loss is: 2.596520719418682e9\n",
      "Current loss is: 2.596259394459274e9\n",
      "Current loss is: 2.596237751431194e9\n",
      "Current loss is: 2.5962360586936483e9\n",
      "Current loss is: 2.596215673667265e9\n",
      "Current loss is: 2.5961948643107786e9\n",
      "Current loss is: 2.5961809709586496e9\n",
      "Current loss is: 2.5961442190313845e9\n",
      "Current loss is: 2.5961056286739807e9\n",
      "Current loss is: 2.596105619507564e9\n",
      "Current loss is: 2.596103484577481e9\n",
      "Current loss is: 2.5960933123980975e9\n",
      "Current loss is: 2.5960797201187096e9\n",
      "Current loss is: 2.596062433121997e9\n",
      "Current loss is: 2.596040785258587e9\n",
      "Current loss is: 2.5960059098094654e9\n",
      "Current loss is: 2.596003744900755e9\n",
      "Current loss is: 2.5960037441849556e9\n",
      "Current loss is: 2.5960033278773518e9\n",
      "Current loss is: 2.5959858027722044e9\n",
      "Current loss is: 2.5959765566101456e9\n",
      "Current loss is: 2.595972631788427e9\n",
      "Current loss is: 2.595877805267224e9\n",
      "Current loss is: 2.595737206421335e9\n",
      "Current loss is: 2.5957096021756134e9\n",
      "Current loss is: 2.595571965416427e9\n",
      "Current loss is: 2.5953115262712293e9\n",
      "Current loss is: 2.595306847746342e9\n",
      "Current loss is: 2.5953033301287107e9\n",
      "Current loss is: 2.5953004439540906e9\n",
      "Current loss is: 2.5952899876891527e9\n",
      "Current loss is: 2.5952783419099603e9\n",
      "Current loss is: 2.5952515423480797e9\n",
      "Current loss is: 2.5952387427773013e9\n",
      "Current loss is: 2.5952378772109246e9\n",
      "Current loss is: 2.5952366229665537e9\n",
      "Current loss is: 2.5952292724405527e9\n",
      "Current loss is: 2.5952280864483013e9\n",
      "Current loss is: 2.5952215332792206e9\n",
      "Current loss is: 2.595221203147339e9\n",
      "Current loss is: 2.595218975002131e9\n",
      "Current loss is: 2.5952114701972094e9\n",
      "Current loss is: 2.5951984820999722e9\n",
      "Current loss is: 2.5951959854465694e9\n",
      "Current loss is: 2.595189084062043e9\n",
      "Current loss is: 2.595188950522058e9\n",
      "Current loss is: 2.595184495531427e9\n",
      "Current loss is: 2.5951468427151694e9\n",
      "Current loss is: 2.595105738141992e9\n",
      "Current loss is: 2.5949709294161325e9\n",
      "Current loss is: 2.5945777629408617e9\n",
      "Current loss is: 2.594472185690656e9\n",
      "Current loss is: 2.5937817954933796e9\n",
      "Current loss is: 2.593604314285702e9\n",
      "Current loss is: 2.593490913959249e9\n",
      "Current loss is: 2.593469835777395e9\n",
      "Current loss is: 2.5934647451912746e9\n",
      "Current loss is: 2.593291022266748e9\n",
      "Current loss is: 2.5932757484031267e9\n",
      "Current loss is: 2.5932596931939206e9\n",
      "Current loss is: 2.5932544625204687e9\n",
      "Current loss is: 2.5930957750788713e9\n",
      "Current loss is: 2.593066230167618e9\n",
      "Current loss is: 2.5929118119994073e9\n",
      "Current loss is: 2.5927102917466044e9\n",
      "Current loss is: 2.5926300724759846e9\n",
      "Current loss is: 2.5925613613206987e9\n",
      "Current loss is: 2.5923987415253263e9\n",
      "Current loss is: 2.5923976023746877e9\n",
      "Current loss is: 2.591947740905977e9\n",
      "Current loss is: 2.591849939432975e9\n",
      "Current loss is: 2.591841327116503e9\n",
      "Current loss is: 2.591764564687335e9\n",
      "Current loss is: 2.5917630777172494e9\n",
      "Current loss is: 2.591752716698236e9\n",
      "Current loss is: 2.5917348701225753e9\n"
     ]
    }
   ],
   "source": [
    "res = Optimization.solve(prob, BFGS(); callback = callback, maxiters=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ = res.u[end-2:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "approximate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimizers = [res.u.depvar[depvars[i]] for i in 1:8]\n",
    "ts = [infimum(d.domain):dt/10:supremum(d.domain) for d in domains][1]\n",
    "u_predict  = [[discretization.phi[i]([t],minimizers[i])[1] for t in ts] for i in 1:3]\n",
    "plot(sol)\n",
    "plot!(ts, u_predict, label = [\"x(t)\" \"y(t)\" \"z(t)\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
